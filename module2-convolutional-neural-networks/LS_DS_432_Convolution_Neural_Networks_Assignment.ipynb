{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 2*\n",
    "# Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "- <a href=\"#p1\">Part 1:</a> Pre-Trained Model\n",
    "- <a href=\"#p2\">Part 2:</a> Custom CNN Model\n",
    "- <a href=\"#p3\">Part 3:</a> CNN with Data Augmentation\n",
    "\n",
    "\n",
    "You will apply three different CNN models to a binary image classification model using Keras. Classify images of Mountains (`./data/mountain/*`) and images of forests (`./data/forest/*`). Treat mountains as the postive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "|Mountain (+)|Forest (-)|\n",
    "|---|---|\n",
    "|![](./data/mountain/art1131.jpg)|![](./data/forest/cdmc317.jpg)|\n",
    "\n",
    "The problem is realively difficult given that the sample is tiny: there are about 350 observations per class. This sample size might be something that you can expect with prototyping an image classification problem/solution at work. Get accustomed to evaluating several differnet possible models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Pre - Trained Model\n",
    "<a id=\"p1\"></a>\n",
    "\n",
    "Load a pretrained network from Keras, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D()\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "```\n",
    "\n",
    "The `include_top` parameter in `ResNet50` will remove the full connected layers from the ResNet model. The next step is to turn off the training of the ResNet layers. We want to use the learned parameters without updating them in future training passes. \n",
    "\n",
    "```python\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "```\n",
    "\n",
    "Using the Keras functional API, we will need to additional additional full connected layers to our model. We we removed the top layers, we removed all preivous fully connected layers. In other words, we kept only the feature processing portions of our network. You can expert with additional layers beyond what's listed here. The `GlobalAveragePooling2D` layer functions as a really fancy flatten function by taking the average of each of the last convolutional layer outputs (which is two dimensional still). \n",
    "\n",
    "```python\n",
    "x = res.output\n",
    "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(1, activation='sigmoid')(x)\n",
    "model = Model(res.input, predictions)\n",
    "```\n",
    "\n",
    "Your assignment is to apply the transfer learning above to classify images of Mountains (`./data/mountain/*`) and images of forests (`./data/forest/*`). Treat mountains as the postive class (1) and the forest images as the negative (zero). \n",
    "\n",
    "Steps to complete assignment: \n",
    "1. Load in Image Data into numpy arrays (`X`) \n",
    "2. Create a `y` for the labels\n",
    "3. Train your model with pretrained layers from resnet\n",
    "4. Report your model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data\n",
    "\n",
    "![skimage-logo](https://scikit-image.org/_static/img/logo.png)\n",
    "\n",
    "Check out out [`skimage`](https://scikit-image.org/) for useful functions related to processing the images. In particular checkout the documentation for `skimage.io.imread_collection` and `skimage.transform.resize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9908>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD99B0>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9A58>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9B00>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9BA8>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9C50>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9D30>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9DD8>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9E80>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9828>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "\n",
    "forestspath = './data/forest/'\n",
    "forests = []\n",
    "valid_files = ['.jpg']\n",
    "for image in os.listdir(forestspath):\n",
    "    ext = os.path.splitext(image)[1]\n",
    "    if ext.lower() not in valid_files:\n",
    "        continue\n",
    "    forests.append(Image.open(os.path.join(forestspath,image)))\n",
    "\n",
    "forests[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF150B8>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BDD9780>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF151D0>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF152B0>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF15358>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF15400>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF154A8>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF15550>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF155F8>,\n",
       " <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=256x256 at 0x1FF9BF156A0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mountainspath = './data/mountain/'\n",
    "mountains = []\n",
    "valid_files = ['.jpg']\n",
    "for image in os.listdir(mountainspath):\n",
    "    ext = os.path.splitext(image)[1]\n",
    "    if ext.lower() not in valid_files:\n",
    "        continue\n",
    "    mountains.append(Image.open(os.path.join(mountainspath,image)))\n",
    "\n",
    "mountains[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y0 = np.zeros(len(forests))\n",
    "y1 = np.ones(len(mountains))\n",
    "y = np.hstack((y0, y1))\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n",
      "[[[0.01176471 0.         0.        ]\n",
      "  [0.01960784 0.00392157 0.        ]\n",
      "  [0.02352941 0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.00392157 0.02352941]\n",
      "  [0.03921569 0.03921569 0.07843137]\n",
      "  [0.         0.         0.03529412]]\n",
      "\n",
      " [[0.01568627 0.         0.        ]\n",
      "  [0.02745098 0.00784314 0.        ]\n",
      "  [0.03137255 0.         0.        ]\n",
      "  ...\n",
      "  [0.04313725 0.04705882 0.06666667]\n",
      "  [0.01176471 0.01568627 0.03529412]\n",
      "  [0.         0.         0.03137255]]\n",
      "\n",
      " [[0.02745098 0.         0.        ]\n",
      "  [0.03529412 0.00392157 0.        ]\n",
      "  [0.03921569 0.         0.        ]\n",
      "  ...\n",
      "  [0.00392157 0.00392157 0.01176471]\n",
      "  [0.         0.         0.00784314]\n",
      "  [0.01568627 0.01568627 0.02352941]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.00784314 0.         0.00392157]\n",
      "  [0.05882353 0.04313725 0.04705882]\n",
      "  [0.03137255 0.01568627 0.01960784]\n",
      "  ...\n",
      "  [0.05490196 0.01960784 0.        ]\n",
      "  [0.03529412 0.         0.        ]\n",
      "  [0.04705882 0.01176471 0.        ]]\n",
      "\n",
      " [[0.00784314 0.00392157 0.02352941]\n",
      "  [0.01176471 0.00784314 0.02745098]\n",
      "  [0.         0.         0.01568627]\n",
      "  ...\n",
      "  [0.01568627 0.         0.        ]\n",
      "  [0.01960784 0.00392157 0.        ]\n",
      "  [0.01568627 0.         0.        ]]\n",
      "\n",
      " [[0.         0.00392157 0.02352941]\n",
      "  [0.         0.00392157 0.02352941]\n",
      "  [0.01568627 0.01960784 0.03921569]\n",
      "  ...\n",
      "  [0.00784314 0.00392157 0.        ]\n",
      "  [0.01176471 0.00392157 0.00784314]\n",
      "  [0.00784314 0.         0.00392157]]]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for image in (forests + mountains):\n",
    "    img_array = np.asarray(image)\n",
    "    X.append(img_array)\n",
    "\n",
    "print(len(X))\n",
    "\n",
    "X = np.array(X)/255\n",
    "\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instatiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model # This is the functional API\n",
    "\n",
    "resnet = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x) # This layer is a really fancy flatten\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(2, activation='sigmoid')(x)\n",
    "model = Model(resnet.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathansokoll/anaconda3/envs/U4-S3-DNN/lib/python3.7/site-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470 samples, validate on 232 samples\n",
      "Epoch 1/20\n",
      "470/470 [==============================] - 140s 297ms/sample - loss: 0.5711 - accuracy: 0.7787 - val_loss: 0.6928 - val_accuracy: 0.5043\n",
      "Epoch 2/20\n",
      "470/470 [==============================] - 135s 287ms/sample - loss: 0.3721 - accuracy: 0.8660 - val_loss: 1.6663 - val_accuracy: 0.4957\n",
      "Epoch 3/20\n",
      "470/470 [==============================] - 120s 255ms/sample - loss: 0.1016 - accuracy: 0.9809 - val_loss: 2.4377 - val_accuracy: 0.4957\n",
      "Epoch 4/20\n",
      "470/470 [==============================] - 120s 254ms/sample - loss: 0.1012 - accuracy: 0.9787 - val_loss: 0.6927 - val_accuracy: 0.4957\n",
      "Epoch 5/20\n",
      "470/470 [==============================] - 121s 257ms/sample - loss: 0.0645 - accuracy: 0.9766 - val_loss: 0.6821 - val_accuracy: 0.5647\n",
      "Epoch 6/20\n",
      "470/470 [==============================] - 120s 255ms/sample - loss: 0.0691 - accuracy: 0.9702 - val_loss: 2.2906 - val_accuracy: 0.4957\n",
      "Epoch 7/20\n",
      "470/470 [==============================] - 120s 255ms/sample - loss: 0.2735 - accuracy: 0.9234 - val_loss: 0.6928 - val_accuracy: 0.5043\n",
      "Epoch 8/20\n",
      "470/470 [==============================] - 120s 255ms/sample - loss: 0.0988 - accuracy: 0.9915 - val_loss: 0.6881 - val_accuracy: 0.5000\n",
      "Epoch 9/20\n",
      "470/470 [==============================] - 120s 254ms/sample - loss: 0.0233 - accuracy: 0.9915 - val_loss: 0.7048 - val_accuracy: 0.5000\n",
      "Epoch 10/20\n",
      "470/470 [==============================] - 120s 254ms/sample - loss: 0.0235 - accuracy: 0.9872 - val_loss: 0.6851 - val_accuracy: 0.5043\n",
      "Epoch 11/20\n",
      "470/470 [==============================] - 120s 255ms/sample - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.6737 - val_accuracy: 0.6121\n",
      "Epoch 12/20\n",
      "470/470 [==============================] - 120s 255ms/sample - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.6597 - val_accuracy: 0.6078\n",
      "Epoch 13/20\n",
      "470/470 [==============================] - 120s 256ms/sample - loss: 6.0340e-04 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 0.6121\n",
      "Epoch 14/20\n",
      "470/470 [==============================] - 120s 254ms/sample - loss: 7.5448e-05 - accuracy: 1.0000 - val_loss: 0.6996 - val_accuracy: 0.5388\n",
      "Epoch 15/20\n",
      "470/470 [==============================] - 121s 258ms/sample - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8071 - val_accuracy: 0.5043\n",
      "Epoch 16/20\n",
      "470/470 [==============================] - 120s 255ms/sample - loss: 0.0623 - accuracy: 0.9830 - val_loss: 0.6833 - val_accuracy: 0.5043\n",
      "Epoch 17/20\n",
      "470/470 [==============================] - 120s 254ms/sample - loss: 0.0320 - accuracy: 1.0000 - val_loss: 0.7296 - val_accuracy: 0.4957\n",
      "Epoch 18/20\n",
      "470/470 [==============================] - 119s 253ms/sample - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 0.6422\n",
      "Epoch 19/20\n",
      "470/470 [==============================] - 119s 254ms/sample - loss: 0.0645 - accuracy: 0.9830 - val_loss: 0.8265 - val_accuracy: 0.4957\n",
      "Epoch 20/20\n",
      "470/470 [==============================] - 120s 254ms/sample - loss: 0.0110 - accuracy: 0.9936 - val_loss: 1.0741 - val_accuracy: 0.5043\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20007e24ac8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          epochs=20, \n",
    "          validation_data=(X_test, y_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well now, that is quite the poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 561 samples, validate on 141 samples\n",
      "Epoch 1/5\n",
      "561/561 [==============================] - 39s 69ms/sample - loss: 0.1216 - accuracy: 0.9715 - val_loss: 1.4221 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "561/561 [==============================] - 35s 62ms/sample - loss: 0.0410 - accuracy: 0.9875 - val_loss: 2.9222 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "561/561 [==============================] - 35s 62ms/sample - loss: 0.0646 - accuracy: 0.9697 - val_loss: 0.9962 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/5\n",
      "561/561 [==============================] - 39s 70ms/sample - loss: 0.0723 - accuracy: 0.9733 - val_loss: 0.2627 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "561/561 [==============================] - 38s 68ms/sample - loss: 0.0333 - accuracy: 0.9893 - val_loss: 0.4859 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f980e9c6f60>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN Model\n",
    "\n",
    "In this step, write and train your own convolutional neural network using Keras. You can use any architecture that suits you as long as it has at least one convolutional and one pooling layer at the beginning of the network - you can add more if you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_39 (Conv2D)           (None, 254, 254, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 127, 127, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 125, 125, 32)      18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 60, 60, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 30, 30, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 28, 28, 8)         1160      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 12, 12, 4)         292       \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 116,958\n",
      "Trainable params: 116,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setup Architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(16, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(8, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Conv2D(4, (3,3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 470 samples, validate on 232 samples\n",
      "Epoch 1/20\n",
      "470/470 [==============================] - 26s 56ms/sample - loss: 0.5109 - accuracy: 0.7596 - val_loss: 0.8200 - val_accuracy: 0.6853\n",
      "Epoch 2/20\n",
      "470/470 [==============================] - 26s 55ms/sample - loss: 0.3109 - accuracy: 0.8638 - val_loss: 0.2510 - val_accuracy: 0.8966\n",
      "Epoch 3/20\n",
      "470/470 [==============================] - 26s 55ms/sample - loss: 0.2594 - accuracy: 0.9000 - val_loss: 0.2075 - val_accuracy: 0.9138\n",
      "Epoch 4/20\n",
      "470/470 [==============================] - 26s 55ms/sample - loss: 0.1897 - accuracy: 0.9298 - val_loss: 0.1906 - val_accuracy: 0.9267\n",
      "Epoch 5/20\n",
      "470/470 [==============================] - 26s 56ms/sample - loss: 0.1750 - accuracy: 0.9362 - val_loss: 0.1760 - val_accuracy: 0.9310\n",
      "Epoch 6/20\n",
      "470/470 [==============================] - 26s 56ms/sample - loss: 0.1535 - accuracy: 0.9532 - val_loss: 0.1998 - val_accuracy: 0.9224\n",
      "Epoch 7/20\n",
      "470/470 [==============================] - 26s 56ms/sample - loss: 0.1985 - accuracy: 0.9298 - val_loss: 0.1955 - val_accuracy: 0.9267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x200082f9a20>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model\n",
    "stop= EarlyStopping(monitor='val_accuracy', min_delta=.01, patience=3)\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=20, \n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[stop]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like my validation accuracy ended up at just shy of 93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 561 samples, validate on 141 samples\n",
      "Epoch 1/20\n",
      "561/561 [==============================] - 31s 55ms/sample - loss: 0.1789 - accuracy: 0.9358 - val_loss: 0.0607 - val_accuracy: 0.9858\n",
      "Epoch 2/20\n",
      "561/561 [==============================] - 30s 53ms/sample - loss: 0.1491 - accuracy: 0.9376 - val_loss: 0.9937 - val_accuracy: 0.6028\n",
      "Epoch 3/20\n",
      "561/561 [==============================] - 30s 53ms/sample - loss: 0.1969 - accuracy: 0.9234 - val_loss: 0.0945 - val_accuracy: 0.9787\n",
      "Epoch 4/20\n",
      "561/561 [==============================] - 30s 53ms/sample - loss: 0.1594 - accuracy: 0.9376 - val_loss: 0.3757 - val_accuracy: 0.8865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20012df8f60>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model\n",
    "model.fit(X, y,\n",
    "          #X_train, y_train, \n",
    "          epochs=20, \n",
    "          validation_split=0.2,\n",
    "          shuffle=True,\n",
    "          callbacks=[stop]\n",
    "          #validation_data=(X_test, y_test))\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 215, 215, 32)      9632      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 39, 39, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 64)          102464    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5184)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 64)                331840    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 495,265\n",
      "Trainable params: 495,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 561 samples, validate on 141 samples\n",
      "Epoch 1/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.2667 - accuracy: 0.9073 - val_loss: 0.1186 - val_accuracy: 0.9858\n",
      "Epoch 2/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.2046 - accuracy: 0.9073 - val_loss: 0.3342 - val_accuracy: 0.8511\n",
      "Epoch 3/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.1778 - accuracy: 0.9287 - val_loss: 0.2746 - val_accuracy: 0.8723\n",
      "Epoch 4/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.1681 - accuracy: 0.9323 - val_loss: 0.8487 - val_accuracy: 0.5957\n",
      "Epoch 5/5\n",
      "561/561 [==============================] - 18s 32ms/sample - loss: 0.1606 - accuracy: 0.9394 - val_loss: 0.3903 - val_accuracy: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f97388777f0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CNN Model with Image Manipulations\n",
    "## *This a stretch goal, and it's relatively difficult*\n",
    "\n",
    "To simulate an increase in a sample of image, you can apply image manipulation techniques: cropping, rotation, stretching, etc. Luckily Keras has some handy functions for us to apply these techniques to our mountain and forest example. Check out these resources to help you get started: \n",
    "\n",
    "1. [Keras `ImageGenerator` Class](https://keras.io/preprocessing/image/#imagedatagenerator-class)\n",
    "2. [Building a powerful image classifier with very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State Code for Image Manipulation Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "# Resources and Stretch Goals\n",
    "\n",
    "Stretch goals\n",
    "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
    "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
    "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
    "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
    "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
    "\n",
    "Resources\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
    "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
    "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
    "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
    "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S3-DL (Python3)",
   "language": "python",
   "name": "u4-s2-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
