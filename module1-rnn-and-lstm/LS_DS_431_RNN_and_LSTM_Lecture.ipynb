{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ldr0HZ193GKb"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 1*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) (Prepare)\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "## Overview\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "# Neural Networks for Sequences (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "Sequences come in many shapes and forms from stock prices to text. We'll focus on text, because modeling text as a sequence is a strength of Neural Networks. Let's start with a simple classification task using a TensorFlow tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad Sequences (samples x time)\n",
      "x_train shape:  (25000, 80)\n",
      "x_test shape:  (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad Sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "print('x_test shape: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 48s 2ms/sample - loss: 0.2472 - accuracy: 0.9049 - val_loss: 0.3858 - val_accuracy: 0.8360\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 53s 2ms/sample - loss: 0.2282 - accuracy: 0.9136 - val_loss: 0.3957 - val_accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "unicorns = model.fit(x_train, y_train,\n",
    "          batch_size=1024, # Bigger batch sizes result in quicker training but less learning\n",
    "          epochs=2, \n",
    "          validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(unicorns.history['loss'])\n",
    "plt.plot(unicorns.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use an Keras LSTM for a classicification task on the *Sprint Challenge*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "# LSTM Text generation with Keras (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r', encoding='utf-8') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here’s their advice to upgrade your game:\\n\\n1. Be quiet and listen\\n\\nRegistering and understanding noise is a huge key to helping you win. If you listen closely enough, you can predict what the enemy will do. Likewise, manage your own noise so you don’t make your movements so obvious.\\n\\nAD\\n\\nUbisoft developed its own sound propagation system to make the game as realistic as possible. In typical games, you’ll hear a noise from an adjacent room, but it’s muffled. In Siege, noise travels from person to person through space in the shortest way possible, bouncing off walls and entering through doorways.\\n\\nAD\\n\\nNiclas “Pengu” Mouritzen, flex player for the G2 Esports Siege team, said managing noise is something he and his teammates constantly work on. For example, jumping with your drone is quite loud, making it easy for enemies to track it down and destroy it. Mouritzen cautioned to only jump away if your drone is in danger of being destroyed.\\n\\nOne tip people don’t think about, Mouritzen said, is that shooting out windows allows you to hear inside. Sound won’t filter through the room unless the window is destroyed. It’s the small things that are make or break in a realistic game like Siege.\\n\\nAD\\n\\n2. Learn the maps until you can see them blindfolded\\n\\nRainbow Six Siege features highly dynamic, destructible, multilevel maps. You can get shot from just about anywhere, which can make it frustrating to play for beginners.\\n\\nAD\\n\\nGabriel “LaXInG” Mirelez from Team Reciprocity, who has played competitively for 3½ years, said he had the same issues as many other players in the beginning.\\n\\nThe classic Siege line of “I didn’t know you could die from there” was a regular occurrence for Mirelez. But the difference between him and many others, he said, was the drive to improve.\\n\\n“If you want to improve, you really have to want it,” Mirelez said. Doing the same thing over and over again isn’t going to cut it in a game like Siege.\\n\\nAD\\n\\nHe recommends watching professional gameplay and high-level streamers to understand the best angles to take in a map. Keep yourself protected as much as possible while giving yourself the best angle to scope your enemies.\\n\\n3. Equip the right scope and attachments to fit the occasion\\n\\nPart of finding success in Siege is knowing which operator fits your situation and play style. More than that, you’ll need to figure out which scopes to use when you are defending or attacking.\\n\\nAD\\n\\nThe game narrows down the scopes you can use in particular situations. Attackers and only a few defenders have the option of using ACOG, a scope with 2x magnification, but selecting it all the time isn’t the best option. It forces you to hold and angle and rely on the enemy to make a mistake.\\n\\nAD\\n\\nBut in close-quarters combat, Mouritzen said, 1x sights, such as reflex and red dot, are king. They let you take things into your own hands and improve upon fragging (kills). However, if you are playing a support operator such as Thermite, it’s best to hang back and hold down an angle with an ACOG sight.\\n\\n4. Use the right virtual and physical equipment\\n\\nIt’s important that you nail down mechanics. In Siege, you’ll need to be able to do a 180 on a dime and shoot first.\\n\\nYour mouse is the key to it all. It’s important to find a mouse that fits your hand comfortably. That won’t necessarily be the one that the pros are using, Mouritzen said. Sometimes they’re using a mouse that’s part of a sponsorship deal, so it may not work for you.\\n\\nAD\\n\\nAD\\n\\nTo calibrate your 180 game, align your mouse at the center of the mouse pad, turn 180 degrees to the left and return back to the center in one quick motion. If you can do that, you should have a good setup, Mouritzen said.\\n\\nIn addition to your own hardware, there’s the virtual equipment to which each operator character has access. Blitz uses a riot shield that acts as a flashbang, Echo has a quadcopter drone that can disorient people, and Kaid electrifies shields, hatches and barbed wire with his Electroclaw. Understanding how this equipment synergizes with the rest of your team will help you to victory.\\n\\n5. Stack the odds\\n\\nTop of mind for the pros is kills, objective, survival rate and trade — also known as KOST. Some of the terms are obvious, like kills refers to eliminating opponents and the objective is how you approach planting/disarming the bomb or holding an area. Survival rate is about improving your odds at living through an engagement, while trades refer to making opponents pay if they take out one of your teammates. Each of those concepts factors into a player’s decision-making. What this really boils down to, Mouritzen said, is doing whatever it takes to give your team an advantage.\\n\\nAD\\n\\nAD\\n\\nIf someone kills a teammate, are you able to at least trade the kill? Can you bring in another person to help clear a room? How can you leverage a numbers advantage? It all has to do with odds.\\n\\nTaking a 50-50 gunfight might work, but it can just as well cost you the game if you lose. That’s probably not a worthwhile fight to pick. Efficient allocation of resources (teammates, drones, grenades, equipment, etc.) given the situation can change the odds to 80-20, the pros say, and help you confidently take a fight.\\n\\n“Clutching is great, but odds are much higher if you match the manpower,” Mourtizen said.\\n\\nIt’s a team game, after all.\\n\\nRead more from The Post:\\n\\nAD'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "# Gather all text into a single string\n",
    "# Why? 1. See all possible characters 2. For training / splitting later\n",
    "text = \" \".join(data)\n",
    "\n",
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  178374\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65,\n",
       " 5,\n",
       " 42,\n",
       " 98,\n",
       " 77,\n",
       " 101,\n",
       " 76,\n",
       " 5,\n",
       " 36,\n",
       " 37,\n",
       " 26,\n",
       " 101,\n",
       " 5,\n",
       " 84,\n",
       " 37,\n",
       " 26,\n",
       " 101,\n",
       " 98,\n",
       " 5,\n",
       " 62,\n",
       " 37,\n",
       " 102,\n",
       " 77,\n",
       " 21,\n",
       " 76,\n",
       " 5,\n",
       " 37,\n",
       " 20,\n",
       " 20,\n",
       " 77,\n",
       " 21,\n",
       " 76,\n",
       " 26,\n",
       " 5,\n",
       " 20,\n",
       " 116,\n",
       " 101,\n",
       " 116,\n",
       " 102,\n",
       " 102]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A white Fort Worth police officer fatally sho'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 121)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_char[65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)), dropout=0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 3.2693\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \" just play big to make the game easier f\"\n",
      " just play big to make the game easier fnurentfsi aedomasu n.Piftg\n",
      "dng ’”ogi cisepnpieitxiietetfp.uamtii dnaheEri ty hs clgrhbtPmsdnd tddbmlee rpxoinn nnd meetmdhsewtrEnwgnrttol pikileii i4 ng eit laln \n",
      "l sy gsarr.emeleoeoDwont arMttyftn gosmi\n",
      "rbtoomto itt nilfsytt foohabc”herlashtvlfieroto e reaaxWtl biiS Sedg re Dd tat ab  teln it soecietoa n, e al-es,hK g uo aSstu lmt hesibctm u th eocsce  i pn wnLow cItpbt0  ourtotie g  eerils fo .e\n",
      "178374/178374 [==============================] - 113s 634us/sample - loss: 3.2692\n",
      "Epoch 2/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.9809\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"vestigate a Democratic challenger, Trump\"\n",
      "vestigate a Democratic challenger, Trumpapeas fmo mef neoricavt iaEnT icra ectullraLhlos ui-ec uhes  o s tnmrPhe\n",
      " to ide amae cet at -ofmh nt uatartsrfdgtgy  J  eUss ar  abeo inl srh, we tms\n",
      " lodity chom ndanu icunmeth Wcea zedHe’atabls t,easuianA. pey yec arind tof\n",
      "d n—rtomTel asddicu Are auutKo--bdea gsoh khuret orbso teautuy ”heuiling de \n",
      "tes rnen  aa aJtco iutf mlygouise0vrC(bc satart  aa.y hue\n",
      ")e, SktYaiablib acoceidadcinialem(ff s\n",
      "178374/178374 [==============================] - 125s 702us/sample - loss: 2.9808\n",
      "Epoch 3/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.7749\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"tremely interesting.\n",
      "\n",
      "AD\n",
      "\n",
      "Think of closi\"\n",
      "tremely interesting.\n",
      "\n",
      "AD\n",
      "\n",
      "Think of closinUlN, cuerefasgsacec horserltne . k aalsnMn cer yain trl udek, adet Whes Han oyriar w-d\n",
      "Ace iou his gheyerntdeaAcey armkytoritnifhen Tr pons cxonato ternd uiperellinpia tong..\n",
      "\n",
      "Nusky bed,ankeie nd mothor piogs frint t che byloy uere roni lekhecith d athede. (on oo.  ntdorho sicins aotors ad ched oorger osuar n rhar cheren attor “erersaotd ac hhg lnipal Er  id ikpuge guUtwe-d.foxte)l bwr sammd oite\n",
      "178374/178374 [==============================] - 120s 672us/sample - loss: 2.7750\n",
      "Epoch 4/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.6751\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"ho called the situation “a total s---sto\"\n",
      "ho called the situation “a total s---sto hevhe tho troun fony axwenm O zoid tue z., kuldsasour Yle nans Ted Em an’s’n sfy th yet, ank, camabed whe ty  tov yolaf Had sun thor lo dhes rusas  ortherDs Nolal-srde\n",
      "t9emeTon og chFyap4ic tho tt or Mistof ehe coltgup cingeg he s]rondrelt onitis a th erlty Pherutevind oe Wuned wame neslo Niyeth. \n",
      "0tyre con\n",
      "\n",
      "\n",
      "romkyret wks ndBawast EesC tor uny thantiged tuneefatteecolnM. BeninlaliTpupbmh  ki pae \n",
      "178374/178374 [==============================] - 121s 676us/sample - loss: 2.6752\n",
      "Epoch 5/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.6176\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"e you’re on your own two feet, Pappa sai\"\n",
      "e you’re on your own two feet, Pappa saity he coaCaminn poulle aryy, io gisoust the owcee Hornt Sed,, Jgathary c o thohegiok is ll bay nelyrdar. RD Haid ha hegherib rinn foriloplether bntsed nrell aed dirivasenig tod al tewe ter\n",
      "\n",
      "mSstowllymuln Candem; ttlmion, horto ppiyrang stpeptouses cn mifaor mley oof ant on ondain, sr otsthe \n",
      "Sicereed ter “f acd everi xoide’s ts, to sertin so anda coul: gerye Frrptertcoiles shes, ta, priyse\n",
      "azedisd\n",
      "178374/178374 [==============================] - 122s 683us/sample - loss: 2.6176\n",
      "Epoch 6/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.5807\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"sis of previously unpublished Russian Ai\"\n",
      "sis of previously unpublished Russian Aind teid mam ibcath Onijf on jugders The pon lo bired morceed4,ninae d.f tierse lo thut tho Hese stoicad cud-pashesworee inmy rn ntwem tor dowa foriy dint duot ampiplary side goundeime toayes anee fac matdeael7ty” cawe ring a periesins Ife chquml ghat cedabige. the pimer ingonithvio thitme s allliche tiolge this pofot peed idince y ntiy in arr pneconas’ momorasicos.\n",
      "\n",
      "Dipnid the to tde vorea, Prosyi\n",
      "178374/178374 [==============================] - 113s 634us/sample - loss: 2.5806\n",
      "Epoch 7/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.5499\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"\n",
      "\n",
      "AD\n",
      "\n",
      "“As the President of the United St\"\n",
      "\n",
      "\n",
      "AD\n",
      "\n",
      "“As the President of the United Stasl yntkerad/ nher as lesisger soak th arnis yroe nros lenss anat roxtit ors, w int,shed whaseichrcresolo tick Preacred on Mheine gos, Fotseitved to bedi es,ahe heind whe teeliat one forupJos riltho Wimiele poy eno add’t Pampyry ans incuilistyetiy thow. Aien f naes, and Buthirt, bor lrge covn bus aadsy satfit ins the yersting wins fr. Sh ber afre efw one, ared ghn shaice to nag louevicrw. hag ide \n",
      "178374/178374 [==============================] - 133s 748us/sample - loss: 2.5501\n",
      "Epoch 8/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.5246\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"recently moved to Pittsburgh. Only when \"\n",
      "recently moved to Pittsburgh. Only when gfiend MtremicaterodeiMlo U1.)\n",
      "\n",
      "Ticbhel oessint ions, rarent.\n",
      "Theree1 hice nomens dut vare sol cosing by As.\n",
      "\n",
      "Thol blap arenchumt haoforitinith kieqy the gor hh in bive ghw. Dvaind ornnizind io ew and wing, wismunv rillouck at ar con 3o (Linkmat tred drmeeisutta ton pacense. Daplyracen to inginn that The his 1h urtxnemorinN Toubem biach We we colitame; Is DCwomer.\n",
      "\n",
      "Wist\n",
      "uhedeott Fone cung’s orcant\n",
      "178374/178374 [==============================] - 115s 643us/sample - loss: 2.5248\n",
      "Epoch 9/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.5023\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \", N.Y., despite being headquartered in C\"\n",
      ", N.Y., despite being headquartered in Capten ssas beizinchnt, is whe tramvilll fro thing ron sosikis Twind he dis linpers tht tpobl. ploy horen/le. Aremis' nhe moteut tickif thal Ractithe ordest. gno barina ncovbey and. AF oregouces afsrng anedreviw coved in tixe dezivin tor  o, praind E frxthatus, & tir the Is e vessidat wo) masd tho be vove ong sUtaewe the the prsthau his. baning blo dese castorgel itmemst te tounds an eur os op th m\n",
      "178374/178374 [==============================] - 117s 657us/sample - loss: 2.5022\n",
      "Epoch 10/10\n",
      "178176/178374 [============================>.] - ETA: 0s - loss: 2.4843\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"o blocks from their apartment, a comfort\"\n",
      "o blocks from their apartment, a comfort af antsie ing llidean pedenie/2dels, aherecinh ona ing tury four, arzwided Ror omkivis vesmyo: in O— inl, Caser Pavicelybouthiestad saclsy.”\n",
      "\n",
      "Steie wan shat anf llyorempren Swingoregious’v  ffrded allly yercuth th mox ar Trupe:, on theshve the th t ondoysed ouf a gh e, or that Wrumaleve sedea ramz” Is in to prlenting thom has wcithtegnt calnksovoluriwet te whin ptibe teraatc pof whotnberpor, beck\n",
      "178374/178374 [==============================] - 111s 623us/sample - loss: 2.4842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1533088a4a8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=1024,\n",
    "          epochs=10,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will be expected to use a Keras LSTM to generate text on today's assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- <a href=\"#p1\">Part 1: </a>Describe Neural Networks used for modeling sequences\n",
    "    * Sequence Problems:\n",
    "        - Time Series (like Stock Prices, Weather, etc.)\n",
    "        - Text Classification\n",
    "        - Text Generation\n",
    "        - And many more! :D\n",
    "    * LSTMs are generally preferred over RNNs for most problems\n",
    "    * LSTMs are typically a single hidden layer of LSTM type; although, other architectures are possible.\n",
    "    * Keras has LSTMs/RNN layer types implemented nicely\n",
    "- <a href=\"#p2\">Part 2: </a>Apply a LSTM to a text generation problem using Keras\n",
    "    * Shape of input data is very important\n",
    "    * Can take a while to train\n",
    "    * You can use it to write movie scripts. :P "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S3-DL (Python3)",
   "language": "python",
   "name": "u4-s2-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
