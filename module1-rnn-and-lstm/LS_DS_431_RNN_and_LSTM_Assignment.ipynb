{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "\n",
    "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
    "\n",
    "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
    "\n",
    "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
    "\n",
    "Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
    "\n",
    "Then, use that trained RNN to generate Shakespearean-ish text. Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
    "\n",
    "Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running. Then, once you've got a proof of concept - start pushing it more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ltj1je1fp5rO"
   },
   "outputs": [],
   "source": [
    "# TODO - Words, words, mere words, no matter from the heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.gutenberg.org/files/100/100-0.txt\n",
      "5783552/5777367 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 5740054 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿\n",
      "Project Gutenberg’s The Complete Works of William Shakespeare, by William\n",
      "Shakespeare\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever.  You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org.  If you are not located in the United States, you’ll\n",
      "have to check the laws of the country where you are located before using\n",
      "this ebook.\n",
      "\n",
      "\n",
      "Title: The Complete Works of William Shakespeare\n",
      "\n",
      "Author: William Shakespeare\n",
      "\n",
      "Release Date: January 1994 [EBook #100]\n",
      "Last Updated: November 7, 2019\n",
      "\n",
      "Language: English\n",
      "\n",
      "Character set encoding: UTF-8\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK THE COMPLETE WORKS OF WILLIAM SHAKESPEARE ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Complete Works of William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      Contents\n",
      "\n",
      "\n",
      "\n",
      "               THE SONNETS\n",
      "\n",
      "           \n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 1000 characters in text\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# Lookup Tables\n",
    "char_int = {c:i for i, c in enumerate(chars)} \n",
    "int_char = {i:c for i, c in enumerate(chars)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:  1148003\n"
     ]
    }
   ],
   "source": [
    "# Create the sequence data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 chars long\n",
    "next_char = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    \n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_char.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences: ', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences),len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_char[i]] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1148003, 40, 108)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)), dropout=0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / 1\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1148003 samples\n",
      "Epoch 1/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.7148\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"ted, either in my mind or\n",
      "  in my means\"\n",
      "ted, either in my mind or\n",
      "  in my means agogr.\n",
      "\n",
      "WTARIINl\n",
      "For ef me icc bant be peat kopFowhm whot thathhl thame_i\n",
      "  r alt sedras\n",
      "\n",
      "OSRINISCmEH. \n",
      " s ithar ue hers, had t y ir ou thous ige\n",
      "I I Cardill  comed yele mhwry ur we thaf ay ve seriof sndso.\n",
      "An’d  foustht nhingdebs wart waw. That unfwhno\n",
      "Band beralad mowea hire shie poar he?\n",
      "     I nfmjeensghontirs r my sofret houIcaull,\n",
      "  Wso male have  eis cousin t at; Oif rr?\n",
      " I  CI\n",
      "1148003/1148003 [==============================] - 738s 643us/sample - loss: 2.7148\n",
      "Epoch 2/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.3089\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \" fear where it should most mistrust;\n",
      "It\"\n",
      " fear where it should most mistrust;\n",
      "Iteroe senefhare’d com ghon I daur fo beatgl\n",
      "Gert leei! Yo t souseotse, Por ot ty ledl iind fut ay\n",
      "Has mame mave ien I kingl theeaare\n",
      "    and Abdrem, INt the yeshit lil yome se,\n",
      "    Lis will. Let, not hocl heirl houss peay,\n",
      "Ma'ls mars oule wie leln to wid his spor\n",
      "Ank anm wonds mein ane liot dous.\n",
      "And rnst most fiou.\n",
      "    CMSOTONO\n",
      "\n",
      "      ACE Ox, I    I rear you mat Ifarpenone wit te ciffour\n",
      "1148003/1148003 [==============================] - 537s 467us/sample - loss: 2.3088\n",
      "Epoch 3/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.2112\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"\n",
      "\n",
      "             Enter LA PUCELLE, on th\"\n",
      "\n",
      "\n",
      "             Enter LA PUCELLE, on thi onall frres I fremimestampat: thoukh be a ove;\n",
      "    Maston, inceaaties eattels galt you theore.\n",
      "\n",
      "FETHANT.\n",
      "I cadct, fouchs uloranry.\n",
      "\n",
      "GHaN, IORUE. Matear.\n",
      "\n",
      "MWMICI.\n",
      "Dill corish of east, andenou Haich’s sartsionble haln, Buagiande\n",
      "H s  Joum to dallus Plave, her ghadnm.\n",
      "   JACONT. Te tire a anghisw rn, cht mex\n",
      "    Waase betcre for wht, you sping 'e poar,\n",
      "Ot and hard ondin aodart-bansid._]\n",
      "1148003/1148003 [==============================] - 530s 462us/sample - loss: 2.2112\n",
      "Epoch 4/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.1503\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \" I'll drown you in the malmsey-butt with\"\n",
      " I'll drown you in the malmsey-butt with angeri,\n",
      "But the freg my you, his mot. My why a mast.\n",
      "Trepokn hed majeshie, Stall flad, To sins yois wn wight l’cl greind\n",
      "\n",
      "STEPIA.\n",
      "Whet he, leverswextt a dowe must abad un,\n",
      "    Whace hunoont am’er's, be sale hish be ent ;owem\n",
      "nat anner ourd!\n",
      "\n",
      " ETRINTE..\n",
      "Ore is my lontes speaunty fule, and rese lask;\n",
      "    Whitio coudy to notin mact. My mengave then thane\n",
      "Whothas am shay, athis meghing lev\n",
      "1148003/1148003 [==============================] - 525s 457us/sample - loss: 2.1503\n",
      "Epoch 5/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.1036\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"RD. Philip, my liege, so is my name begu\"\n",
      "RD. Philip, my liege, so is my name beguts,\n",
      "    Whou. Ixpt thise to hame you hearlangl you tove ofr;\n",
      "    Det shon couch beiech hoth my lik, whou hegl me?\n",
      "To tRedd thus no s oats youn ouctristie me whe \n",
      "hold gight, my ffn.\n",
      "\n",
      " ACnt LuRe. Hivinavids!\n",
      "\n",
      "STENS.\n",
      "E’pich as a levire of dcem it whoo dungly,\n",
      "D toneld en thit the Capgirs, of thoug what,\n",
      "Howr seallest I byowel fue, agl mend tand dig leatey thar,\n",
      "The wit hcid The flakines’d\n",
      "1148003/1148003 [==============================] - 526s 459us/sample - loss: 2.1036\n",
      "Epoch 6/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.0697\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \" lord’s. Will go yet?\n",
      "Force me to keep \"\n",
      " lord’s. Will go yet?\n",
      "Force me to keep harusiy uvens’d as a bay\n",
      "to net my a willsenk, I way bet him lave ue hanu,\n",
      "    Fe arable of to fliceterme of coneere;\n",
      "    Than arse me alil.\n",
      "  LIND. Aldofer haved ope; and book takn with torther.\n",
      "\n",
      "Y_TO..\n",
      "I am.\n",
      "\n",
      "\n",
      "    DIRON-TRS. Twor hor's apothing, and you badess.\n",
      "\n",
      "  CINSCLA. Nary, pripe didioss matio! to And\n",
      "    Metore the sunor-thous asf the Kits of\n",
      "that jeatemofe; Mad- fotredres! d\n",
      "1148003/1148003 [==============================] - 525s 457us/sample - loss: 2.0697\n",
      "Epoch 7/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.0405\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"n again.\n",
      "  TIMON. I have a tree, which \"\n",
      "n again.\n",
      "  TIMON. I have a tree, which me Lupt.\n",
      "  BALLUTEN. O'rme, and a goie in?\n",
      "\n",
      " EGous. Shy dead of Sim Ho, ay ne; see.\n",
      "    Colt and Thou, and, by the cruming-of worbesb,\n",
      "    To Dondew No thealer fortin! I thach I bray gnrem\n",
      "Tilisips to are tame’s of\n",
      "Porsad hagh, or ds augathinl\n",
      "And praine om elor’s ame teapy de ours:\n",
      "We mistel’s what te way free too Taxe.\n",
      "To Horice, from gint: An my seefulits dutkence.\n",
      "Wour peasty, id fr\n",
      "1148003/1148003 [==============================] - 529s 461us/sample - loss: 2.0405\n",
      "Epoch 8/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 2.0160\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"ONBRIDGE, son to Sir Robert Faulconbridg\"\n",
      "ONBRIDGE, son to Sir Robert Faulconbridg\n",
      "Norege alvider is weme. Will subway,\n",
      "    Anduguth than nef Lendilus, Ifwenth; ye’ davire, and son.\n",
      "Awd a will sos it a medsoms! Hease areneingurells,\n",
      "Aese pan of emblens. BUtwAnd unce: In to dendandon.\n",
      "\n",
      " [_Exeunt._]\n",
      "\n",
      "\n",
      "  CINEDISE. Ine thou keele thy lord.\n",
      "  PESSERO. Hou wo, bucclifdd\n",
      "  KING RITHE. Share I lord Prontent, siselfrit is'd be nester;\n",
      "    Artast he my Beit to hiswikst the ab\n",
      "1148003/1148003 [==============================] - 523s 456us/sample - loss: 2.0160\n",
      "Epoch 9/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.9943\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"of honour, may\n",
      "    Make tender of to th\"\n",
      "of honour, may\n",
      "    Make tender of to the filps thy of dysto\n",
      "Madsus taune det futententosh your, with my liat.\n",
      "\n",
      "e[_Beace; And motine the suriuiea,— swaphio, the whilkstiy’s,\n",
      "Spookigh. The nake ot yeve rave of the darchit wo conners\n",
      "                     ESDED4\n",
      "    My tome, Hill berted!                             Enter BERVAWN and herg, and the counh one thould.”   Your chintion thake he hears ercug, it I spere\n",
      "Co have come my i\n",
      "1148003/1148003 [==============================] - 521s 454us/sample - loss: 1.9943\n",
      "Epoch 10/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.9765\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"[_To the Queen._] They were again togeth\"\n",
      "[_To the Queen._] They were again together losh have the has eame not Stied be the knor\n",
      "chilld un the pathed, and hurdour from be; an his and if you.\n",
      "And grest se mus to ig\n",
      "Exet os her not to meed Apinge of Is.\n",
      "\n",
      "FIRST QUEFER.\n",
      "Whow, Latcous cendul othended; in bookirg?\n",
      "Theanor lide lice lown, wr henot sishcre,\n",
      "    An the Kain Flace of like though a; and thou home, in expion beam stey.\n",
      "    Leve corvoturasines.\n",
      "    The King of to\n",
      "1148003/1148003 [==============================] - 575s 501us/sample - loss: 1.9765\n",
      "Epoch 11/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.9594\n",
      "----- Generating text after Epoch: 10\n",
      "----- Generating with seed: \".\n",
      "No, but brown.\n",
      "\n",
      "PANDARUS.\n",
      "Faith, t\"\n",
      ".\n",
      "No, but brown.\n",
      "\n",
      "PANDARUS.\n",
      "Faith, thom conclited for hen toncercuee. Wey\n",
      "swese, but\n",
      "beach ge unch of ears. And nex tomery seean\n",
      "Word an his lotot—dorsters at I for ay\n",
      "\n",
      " [_Exeunt sintur HOS.\n",
      "\n",
      "LARGET.\n",
      "Goicule mozes, Did, a creek. Coutht my, be heauft?\n",
      "\n",
      "DTLETS.\n",
      "I’ter and nor sirck.\n",
      "\n",
      "ACTATINE.\n",
      "My linow!\n",
      "  BURNULE Sun not dud gowry, I and\n",
      "    And lond be siest thour out it Seyven of your nare\n",
      "So for to the entrils’s. au\n",
      "1148003/1148003 [==============================] - 596s 519us/sample - loss: 1.9594\n",
      "Epoch 12/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.9435\n",
      "----- Generating text after Epoch: 11\n",
      "----- Generating with seed: \"y, old man, give me thy hand, away!\n",
      "Kin\"\n",
      "y, old man, give me thy hand, away!\n",
      "Kind we I’rad sould put me upon and the clind:\n",
      "Becedel the courd. Hemeok If whink steio robben!\n",
      "Tow.\n",
      "She\n",
      "hone to make, Folskeet! Dim no! Nase, is as hene oly have hostelf;\n",
      "    By Evin, and dotign a and and ever.\n",
      "  SENERON. Have out, Our crraces when you me you and my lioud,\n",
      "Gwoones; fair that all hurvel; a tonk um corder.\n",
      "\n",
      "KANGUS.\n",
      "O dourtly France of that, and and a Rhaden?\n",
      "\n",
      "ARIMONT.\n",
      "[_A\n",
      "1148003/1148003 [==============================] - 594s 517us/sample - loss: 1.9436\n",
      "Epoch 13/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.9333\n",
      "----- Generating text after Epoch: 12\n",
      "----- Generating with seed: \"reto heel infuse powre, and presse you f\"\n",
      "reto heel infuse powre, and presse you forstilu, in, quiend.\n",
      "Betton assw! Whoy have naght.\n",
      "\n",
      "CORSTIO.\n",
      "Beress of I’d, and sway have net the stall sine\n",
      "The give ont the such is come mo Lener;\n",
      "    But let bronker swouth'd bid fore: good of my suchovis,\n",
      "     wamperar the come, silp in them. Wet on, and breass deline, thun Noverkibe ustcine.\n",
      "    Not mu made knety re you much'st on the themk here of Ander on\n",
      "    and meats it may. “Sha\n",
      "1148003/1148003 [==============================] - 569s 496us/sample - loss: 1.9333\n",
      "Epoch 14/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.9190\n",
      "----- Generating text after Epoch: 13\n",
      "----- Generating with seed: \"ay.\n",
      "\n",
      "ANTIPHOLUS OF EPHESUS.\n",
      "What art \"\n",
      "ay.\n",
      "\n",
      "ANTIPHOLUS OF EPHESUS.\n",
      "What art he wash! Wet, unten upenool,\n",
      "In a rign vodast, then istbeea ition, turntast\n",
      "    And ther, betoft, and whar our nowe.\n",
      "  ALSUANE. Beiehal when just thus baing to hentents.\n",
      "  BUCERTUS. Of non.\n",
      "  CALVsIN. Well thin came the King alous Villes, thet\n",
      "pouth the come hid encesyor tham perice af\n",
      "Iike wo lordst foren\n",
      "me.\n",
      "\n",
      "URFOLS.\n",
      "The goee I in kere nestent,\n",
      "And this it, and men I’ll must but mesi\n",
      "1148003/1148003 [==============================] - 586s 511us/sample - loss: 1.9190\n",
      "Epoch 15/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.9090\n",
      "----- Generating text after Epoch: 14\n",
      "----- Generating with seed: \"anchis’d, and allegiance clear,\n",
      "I shall\"\n",
      "anchis’d, and allegiance clear,\n",
      "I shall have sount thee so fonger, they eas,\n",
      "    Bose tis toull not thou constne tay shall is\n",
      "      And siffles gainpt,\n",
      "    How, nagens boty gind we his coupted, scann;\n",
      "    Shipe in the witite, of in mangiss wr hose,\n",
      "To in rcter least rerefiffed me angow, ay,\n",
      "You hand ary wont. ’Tware eyess no my love,\n",
      "Footh hus unchiones strifes us have thou hank,\n",
      "    Of Swo am would it dack upon with done and s\n",
      "1148003/1148003 [==============================] - 600s 522us/sample - loss: 1.9090\n",
      "Epoch 16/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.8990\n",
      "----- Generating text after Epoch: 15\n",
      "----- Generating with seed: \", counterfeit sad looks,\n",
      "Make mouths up\"\n",
      ", counterfeit sad looks,\n",
      "Make mouths up Asmageer. De-Oft, pript hem and it shours!\n",
      "    I when and pray the, faught in him.\n",
      "  DIPELINE. One ds the wence:\n",
      "    Shall And with hell aboutss  hus have.\n",
      "  MADENIBETHBRLLE. O have the sead'd dave frient stillon mire,\n",
      "    [neyllschove Farnzs a  that,'sunteat]\n",
      "    The lord not butidor memay.                       Entertelf, the CESELS. Gience.\n",
      "  CIRST GHRNCESTENR. Snave!]\n",
      "  V2LOFF. Your e\n",
      "1148003/1148003 [==============================] - 578s 504us/sample - loss: 1.8990\n",
      "Epoch 17/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.8906\n",
      "----- Generating text after Epoch: 16\n",
      "----- Generating with seed: \"like an egg: but thou dost breathe;\n",
      "Has\"\n",
      "like an egg: but thou dost breathe;\n",
      "Has’s compare my bage and erees, as a cond\n",
      "Well pladeed; who sual my toot of ton.\n",
      "    Byst wastule it dirnetted to encestere.\n",
      "Do to use benon and night, of your Que.\n",
      "O   As VilenV.' Werl-ream to las, that kity\n",
      "    Monad, I nou did that thou here a nobten\n",
      "Take my sauning of gualy, and thou pergon,\n",
      "Hot up of our putsecty of Sirm amay;\n",
      "Wrike Is gellt thou daig that I ven'tient,\n",
      "    And this the\n",
      "1148003/1148003 [==============================] - 595s 518us/sample - loss: 1.8906\n",
      "Epoch 18/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.8808\n",
      "----- Generating text after Epoch: 17\n",
      "----- Generating with seed: \".\n",
      "\n",
      "POLONIUS.\n",
      "Affection! Pooh! You spe\"\n",
      ".\n",
      "\n",
      "POLONIUS.\n",
      "Affection! Pooh! You speak not at rese betwed galis’d hulse se\n",
      "    'exysiced this ganesion I’ll him tr under him\n",
      "Make on thou ilith wonr Haughtul fids.\n",
      "Who, what have ded hed fear.\n",
      "Therefore be whreence; we weplesith on ele there.\n",
      "An in mose as a bied upbutter go whelise,\n",
      "Af a womerththing saperon._S deld as bies,\n",
      "That that dot Aumistamin a plomp it to set\n",
      "Touble ver thmate and your wipes onere.\n",
      "\n",
      "CRUSTIVA.\n",
      "You\n",
      "1148003/1148003 [==============================] - 582s 507us/sample - loss: 1.8808\n",
      "Epoch 19/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.8733\n",
      "----- Generating text after Epoch: 18\n",
      "----- Generating with seed: \" give you in this weighty cause?\n",
      "  YORK\"\n",
      " give you in this weighty cause?\n",
      "  YORK. Ir thry this langer were arg ontwept,\n",
      "Of it warce a leirns as flens Perre what me ton;\n",
      "    Whal upuride, who lond, winter him him, agh with him.\n",
      "\n",
      " A dould woll, Carseason, God sir Prassa\n",
      "    IVILIIFORANES PARGIAS S, thoubs that with She bay nom. Here this fromby his liot\n",
      "Lut is for, will to it whis winders, Wises, concausy in your him.\n",
      "  LUCHOMPHOM. How; I at thy griedn to, and parchoo si\n",
      "1148003/1148003 [==============================] - 597s 520us/sample - loss: 1.8733\n",
      "Epoch 20/20\n",
      "1147904/1148003 [============================>.] - ETA: 0s - loss: 1.8663\n",
      "----- Generating text after Epoch: 19\n",
      "----- Generating with seed: \"Boy.\n",
      "\n",
      "BOY.\n",
      "Mine host Pistol, you must\"\n",
      "Boy.\n",
      "\n",
      "BOY.\n",
      "Mine host Pistol, you must heart thou bed, buckef my four Leart,\n",
      "It fall thousant, no howd net\n",
      "to him love toobleed for thee down fram hid?\n",
      "    Mademasseavod'd lefountan-lave rish mooks.\n",
      "  COTSAYANUS. 'Twill be prece the masteies to seeppian.\n",
      "\n",
      "POUTHAUUS.\n",
      "I base is. Teat might helise you hoves in you,\n",
      "  Sake good with here weep yauk; thank is?\n",
      "  MUFFRR. Stake of eeferely doly with your kelp\n",
      "Bener that your foild, \n",
      "1148003/1148003 [==============================] - 538s 468us/sample - loss: 1.8663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x133c6879278>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size=1024,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "## Stretch goals:\n",
    "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
    "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
    "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
    "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
    "- Run on bigger, better data\n",
    "\n",
    "## Resources:\n",
    "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
    "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
    "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
    "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
    "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "U4-S3-DL (Python3)",
   "language": "python",
   "name": "u4-s2-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
